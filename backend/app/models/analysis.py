"""
ë¶„ì„ ê²°ê³¼ ëª¨ë¸
"""

import uuid
from datetime import datetime
from sqlalchemy import Column, String, Integer, DateTime, Text, ForeignKey, JSON, Float
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
from typing import List, Dict, Any

from app.core.database import Base


class Analysis(Base):
    """ë¶„ì„ ê²°ê³¼ ëª¨ë¸"""
    
    __tablename__ = "analyses"
    
    # ê¸°ë³¸ í•„ë“œ
    id = Column(
        UUID(as_uuid=True), 
        primary_key=True, 
        default=uuid.uuid4,
        index=True
    )
    document_id = Column(
        UUID(as_uuid=True), 
        ForeignKey("documents.id", ondelete="CASCADE"),
        nullable=False,
        index=True
    )
    
    # í…ìŠ¤íŠ¸ ë°ì´í„°
    raw_text = Column(Text, nullable=True)
    cleaned_text = Column(Text, nullable=True)
    summary = Column(Text, nullable=True)
    
    # JSON ë°ì´í„°
    keywords = Column(JSON, nullable=True)  # [{"keyword": str, "frequency": int, "importance": float}]
    qa_pairs = Column(JSON, nullable=True)  # [{"question": str, "answer": str, "confidence": float}]
    important_sentences = Column(JSON, nullable=True)  # [{"sentence": str, "importance": float, "page": int}]
    metadata = Column(JSON, nullable=True)  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    
    # ë¶„ì„ ì •ë³´
    ai_model = Column(String(50), nullable=True)  # gpt-4, gpt-3.5-turbo ë“±
    language = Column(String(10), default="ko")
    processing_time = Column(Integer, nullable=True)  # ì´ˆ ë‹¨ìœ„
    
    # í’ˆì§ˆ ë©”íŠ¸ë¦­
    confidence_score = Column(Float, nullable=True)  # 0.0 ~ 1.0
    readability_score = Column(Float, nullable=True)  # ê°€ë…ì„± ì ìˆ˜
    
    # í†µê³„
    total_pages = Column(Integer, nullable=True)
    total_words = Column(Integer, nullable=True)
    total_sentences = Column(Integer, nullable=True)
    total_paragraphs = Column(Integer, nullable=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # ê´€ê³„
    document = relationship("Document", back_populates="analyses")
    exports = relationship("Export", back_populates="analysis", cascade="all, delete-orphan")
    feedback = relationship("Feedback", back_populates="analysis", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<Analysis(id={self.id}, document_id={self.document_id}, ai_model={self.ai_model})>"
    
    @property
    def keyword_list(self) -> List[str]:
        """í‚¤ì›Œë“œ ëª©ë¡ ë°˜í™˜"""
        if not self.keywords:
            return []
        return [item.get("keyword", "") for item in self.keywords if item.get("keyword")]
    
    @property
    def top_keywords(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜"""
        if not self.keywords:
            return []
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_keywords = sorted(
            self.keywords, 
            key=lambda x: x.get("importance", 0), 
            reverse=True
        )
        return sorted_keywords[:limit]
    
    @property
    def question_list(self) -> List[str]:
        """ì§ˆë¬¸ ëª©ë¡ ë°˜í™˜"""
        if not self.qa_pairs:
            return []
        return [item.get("question", "") for item in self.qa_pairs if item.get("question")]
    
    @property
    def high_confidence_qa(self, threshold: float = 0.7) -> List[Dict[str, Any]]:
        """ë†’ì€ ì‹ ë¢°ë„ì˜ Q&A ë°˜í™˜"""
        if not self.qa_pairs:
            return []
        
        return [
            qa for qa in self.qa_pairs 
            if qa.get("confidence", 0) >= threshold
        ]
    
    @property
    def key_sentences(self, limit: int = 5) -> List[str]:
        """í•µì‹¬ ë¬¸ì¥ ë°˜í™˜"""
        if not self.important_sentences:
            return []
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_sentences = sorted(
            self.important_sentences,
            key=lambda x: x.get("importance", 0),
            reverse=True
        )
        return [
            sentence.get("sentence", "") 
            for sentence in sorted_sentences[:limit]
            if sentence.get("sentence")
        ]
    
    def add_keyword(self, keyword: str, frequency: int = 1, importance: float = 0.5):
        """í‚¤ì›Œë“œ ì¶”ê°€"""
        if not self.keywords:
            self.keywords = []
        
        self.keywords.append({
            "keyword": keyword,
            "frequency": frequency,
            "importance": importance
        })
    
    def add_qa_pair(self, question: str, answer: str, confidence: float = 0.8):
        """Q&A ìŒ ì¶”ê°€"""
        if not self.qa_pairs:
            self.qa_pairs = []
        
        self.qa_pairs.append({
            "question": question,
            "answer": answer,
            "confidence": confidence
        })
    
    def add_important_sentence(self, sentence: str, importance: float = 0.8, page: int = 1):
        """ì¤‘ìš” ë¬¸ì¥ ì¶”ê°€"""
        if not self.important_sentences:
            self.important_sentences = []
        
        self.important_sentences.append({
            "sentence": sentence,
            "importance": importance,
            "page": page
        })
    
    def get_summary_stats(self) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ í†µê³„ ë°˜í™˜"""
        return {
            "total_pages": self.total_pages or 0,
            "total_words": self.total_words or 0,
            "total_sentences": self.total_sentences or 0,
            "total_paragraphs": self.total_paragraphs or 0,
            "keyword_count": len(self.keywords) if self.keywords else 0,
            "qa_count": len(self.qa_pairs) if self.qa_pairs else 0,
            "important_sentence_count": len(self.important_sentences) if self.important_sentences else 0,
            "processing_time": self.processing_time,
            "confidence_score": self.confidence_score,
            "ai_model": self.ai_model,
            "language": self.language
        }
    
    def to_markdown(self) -> str:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        lines = []
        
        # ì œëª©
        lines.append(f"# ë¬¸ì„œ ë¶„ì„ ê²°ê³¼")
        lines.append("")
        
        # ìš”ì•½
        if self.summary:
            lines.append("## ğŸ“‹ ìš”ì•½")
            lines.append(self.summary)
            lines.append("")
        
        # í‚¤ì›Œë“œ
        if self.keywords:
            lines.append("## ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ")
            for keyword in self.top_keywords:
                lines.append(f"- **{keyword['keyword']}** (ì¤‘ìš”ë„: {keyword.get('importance', 0):.2f})")
            lines.append("")
        
        # Q&A
        if self.qa_pairs:
            lines.append("## â“ ì§ˆë¬¸ê³¼ ë‹µë³€")
            for i, qa in enumerate(self.qa_pairs, 1):
                lines.append(f"### Q{i}: {qa['question']}")
                lines.append(f"**A**: {qa['answer']}")
                lines.append("")
        
        # ì¤‘ìš” ë¬¸ì¥
        if self.important_sentences:
            lines.append("## ğŸ’¡ í•µì‹¬ ë¬¸ì¥")
            for sentence in self.key_sentences:
                lines.append(f"- {sentence}")
            lines.append("")
        
        # í†µê³„
        stats = self.get_summary_stats()
        lines.append("## ğŸ“Š ë¶„ì„ í†µê³„")
        lines.append(f"- ì´ í˜ì´ì§€ ìˆ˜: {stats['total_pages']}")
        lines.append(f"- ì´ ë‹¨ì–´ ìˆ˜: {stats['total_words']}")
        lines.append(f"- ì²˜ë¦¬ ì‹œê°„: {stats['processing_time']}ì´ˆ")
        lines.append(f"- AI ëª¨ë¸: {stats['ai_model']}")
        lines.append("")
        
        return "\n".join(lines)

